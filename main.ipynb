{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pathlib\n",
    "\n",
    "pathlib.Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "# 楓葉紅葉日期\n",
    "if not pathlib.Path('data/maple/maple_foliage.csv').exists():\n",
    "    response = requests.get('https://www.data.jma.go.jp/sakura/ruinenchi/015.csv')\n",
    "    response.encoding = 'shift-jis'\n",
    "    with open('data/maple/maple_foliage.csv', 'w', encoding='utf-8') as file:\n",
    "        file.write(response.text)\n",
    "\n",
    "\n",
    "# 楓葉落葉日期\n",
    "if not pathlib.Path('data/maple/maple_fall.csv').exists():\n",
    "    response = requests.get('https://www.data.jma.go.jp/sakura/ruinenchi/016.csv')\n",
    "    response.encoding = 'shift-jis'\n",
    "    with open('data/maple/maple_shedding.csv', 'w', encoding='utf-8') as file:\n",
    "        file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "foliage_data = pd.read_csv('data/maple/maple_foliage.csv', skiprows=1, encoding='utf-8')\n",
    "\n",
    "foliage_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unused_columns(df):\n",
    "    columns = ['番号', '平年値', '最早値', '最早年', '最晩値', '最晩年']\n",
    "    remark_columns = [col for col in df.columns if col.startswith('rm')]\n",
    "    return df.drop(columns=columns + remark_columns)\n",
    "\n",
    "foliage_data = drop_unused_columns(foliage_data)\n",
    "\n",
    "foliage_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foliage_data['地点名'] = foliage_data['地点名'].str.strip()\n",
    "location_names = foliage_data['地点名'].unique()\n",
    "pd.Series(location_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def fetch_history_weather_data(latitude, longtitude, start_date, end_date, variables):\n",
    "    url = (\n",
    "        f'https://archive-api.open-meteo.com/v1/archive?'\n",
    "        f'latitude={latitude}&'\n",
    "        f'&longitude={longtitude}&'\n",
    "        f'start_date={start_date}&'\n",
    "        f'end_date={end_date}&'\n",
    "        f'{variables}&'\n",
    "        'timezone=Asia/Tokyo'\n",
    "    )\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Timeout, retrying...')\n",
    "            sleep(5)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Request failed: {e}')\n",
    "            break\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def download_weather_data(geolocator, location_names, directory):\n",
    "    for location_name in (pbar := tqdm(location_names)):\n",
    "        pbar.set_description('Downloading')\n",
    "        pbar.set_postfix_str(location_name)\n",
    "        \n",
    "        pathlib.Path(f'{directory}/{location_name}').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        location = geolocator.geocode(location_name)\n",
    "        if location is None:\n",
    "            print(f'{location_name} not found')\n",
    "            continue\n",
    "\n",
    "        for year in range(1953, 2024):\n",
    "            if pathlib.Path(f'{directory}/{location_name}/{year}.json').exists():\n",
    "                continue\n",
    "            response = fetch_history_weather_data(\n",
    "                location.latitude, \n",
    "                location.longitude, \n",
    "                f'{year}-09-01', \n",
    "                f'{year}-12-31', \n",
    "                'daily=temperature_2m_max,temperature_2m_min,temperature_2m_mean,daylight_duration,precipitation_sum'\n",
    "            )\n",
    "\n",
    "            if response is None:\n",
    "                print(f'Failed to fetch data for {location_name} in {year}')\n",
    "                return\n",
    "\n",
    "            # 確認是否有錯誤 ( 如: API 每小時的限制 )\n",
    "            if response.get('error'):\n",
    "                print(response.get('reason'))\n",
    "                return\n",
    "\n",
    "            with open(f'{directory}/{location_name}/{year}.json', 'w', encoding='utf-8') as file:\n",
    "                file.write(json.dumps(response))\n",
    "            \n",
    "            sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"kaede\")\n",
    "\n",
    "download_completed = True\n",
    "\n",
    "location_names = np.array_split(location_names, 4)\n",
    "\n",
    "id = 2\n",
    "if not download_completed:\n",
    "    download_weather_data(geolocator, location_names[id], 'data/weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_index(year, date):\n",
    "    \"\"\"\n",
    "    獲取日期在某段時間的索引\n",
    "    \"\"\"\n",
    "    start_date = f'{year}-09-01'\n",
    "    end_date = f'{year}-12-31'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "\n",
    "    return date_range.get_loc(date)\n",
    "\n",
    "def number_to_date(num):\n",
    "    \"\"\"\n",
    "    將數字轉換為日期格式\n",
    "    \"\"\"\n",
    "    return f'{num // 100:02d}-{num % 100:02d}'\n",
    "\n",
    "def get_weather_data(data, metric, index, offset, mode=None):\n",
    "    \"\"\"\n",
    "    取得天氣資料中某個指標的指定範圍平均值、最大值或最小值\n",
    "\n",
    "    Args:\n",
    "        data (dict): 天氣資料\n",
    "        metric (str): 指標名稱\n",
    "        index (int): 日期索引\n",
    "        offset (int): 日期偏移量\n",
    "        mode (str): 計算模式 (mean, max, min)\n",
    "    Returns:\n",
    "        float: 計算結果\n",
    "    \"\"\"\n",
    "    # 確認是否有足夠資料\n",
    "    if index < offset:\n",
    "        return None\n",
    "    \n",
    "    if offset != 0: # 如果偏移量不為 0 則進行不同模式的計算\n",
    "        if mode == 'mean':\n",
    "            return sum(data['daily'][metric][index-offset:index]) / (offset + 1)\n",
    "        elif mode == 'max':\n",
    "            return max(data['daily'][metric][index-offset:index])\n",
    "        elif mode == 'min':\n",
    "            return min(data['daily'][metric][index-offset:index])\n",
    "    else:\n",
    "        return data['daily'][metric][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 天氣指標\n",
    "metrics = ['temperature_2m_max', 'temperature_2m_min', 'temperature_2m_mean', 'daylight_duration', 'precipitation_sum']\n",
    "\n",
    "# 將天氣指標轉換為縮寫\n",
    "metric_mapping = {\n",
    "    'temperature_2m_max': 't',\n",
    "    'temperature_2m_min': 't',\n",
    "    'temperature_2m_mean': 't',\n",
    "    'daylight_duration': 'd',\n",
    "    'precipitation_sum': 'p'\n",
    "}\n",
    "\n",
    "# 週期所對應的日數 (offset)\n",
    "periods = {\n",
    "    '': 0,\n",
    "    '1w': 6,\n",
    "    '2w': 13\n",
    "}\n",
    "\n",
    "# 計算模式\n",
    "mode_mapping = {\n",
    "    'temperature_2m_max': ['max'],\n",
    "    'temperature_2m_min': ['min'],\n",
    "    'temperature_2m_mean': ['mean'],\n",
    "    'daylight_duration': ['max', 'mean', 'min'],\n",
    "    'precipitation_sum': ['max', 'mean', 'min']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = [\n",
    "    'latitude',\n",
    "    'd', 'p', 't_max', 't_min', 't_mean',\n",
    "    't_1w_max', 't_1w_min', 't_1w_mean',\n",
    "    't_2w_max', 't_2w_min', 't_2w_mean',\n",
    "    'd_1w_max', 'd_1w_min', 'd_1w_mean',\n",
    "    'd_2w_max', 'd_2w_min', 'd_2w_mean',\n",
    "    'p_1w_max', 'p_1w_min', 'p_1w_mean',\n",
    "    'p_2w_max', 'p_2w_min', 'p_2w_mean'\n",
    "]\n",
    "\n",
    "data = pd.DataFrame(columns=data_column)\n",
    "\n",
    "record_data = foliage_data.to_dict(orient='records')\n",
    "for record in (pbar := tqdm(record_data)):\n",
    "    pbar.set_postfix_str(record['地点名'])\n",
    "    location_name = record['地点名']\n",
    "    location = geolocator.geocode(location_name)\n",
    "\n",
    "    for year in range(1953, 2024):\n",
    "        foliage_day = record[str(year)]\n",
    "        if foliage_day == 0 or len(str(foliage_day)) == 3: # 沒有資料\n",
    "            continue\n",
    "\n",
    "        data_index = get_date_index(year, f'{year}-{number_to_date(foliage_day)}')\n",
    "\n",
    "        # 讀取天氣資料\n",
    "        with open(f'data/weather/{location_name}/{year}.json', 'r', encoding='utf-8') as file:\n",
    "            weather_data = json.load(file)\n",
    "        \n",
    "        # 取紅葉當天的資料當作 true，前 6 天的資料當作 false\n",
    "        for index in range(data_index - 6, data_index + 1):\n",
    "            new_row = { \n",
    "                'latitude': location.latitude, # 緯度\n",
    "                'is_foliage': 1 if index == data_index else 0, # 是否為紅葉當天\n",
    "            }\n",
    "\n",
    "            # 對於所有的組合 (metric, period, mode)\n",
    "            for metric in metrics:\n",
    "                for period, offset in periods.items():\n",
    "                    for mode in mode_mapping[metric]:\n",
    "                        # 跳過沒有用的組合 Ex: d_max, d_min\n",
    "                        if metric in ['daylight_duration', 'precipitation_sum'] and mode in ['max', 'min'] and offset == 0:\n",
    "                            continue\n",
    "\n",
    "                        # 設定欄位名稱\n",
    "                        if offset == 0 and metric in ['daylight_duration', 'precipitation_sum']:\n",
    "                            key = f'{metric_mapping[metric]}' # Ex: d, p\n",
    "                        elif offset == 0:\n",
    "                            key = f'{metric_mapping[metric]}_{mode}' # Ex: t_max, t_min, t_mean\n",
    "                        else:\n",
    "                            key = f'{metric_mapping[metric]}_{period}_{mode}'\n",
    "                        \n",
    "                        # 取得資料\n",
    "                        value = get_weather_data(weather_data, metric, index, offset, mode)\n",
    "                        if value is None:\n",
    "                            continue\n",
    "                        new_row[key] = value\n",
    "\n",
    "            new_row = pd.DataFrame(new_row, index=[0])\n",
    "            data = pd.concat([data, new_row], ignore_index=True)\n",
    "        \n",
    "data.to_csv('data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'subsample': 0.9, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 2000, 'min_child_weight': 5, 'max_depth': 20, 'learning_rate': 0.1, 'gamma': 0.2, 'colsample_bytree': 0.8}\n",
      "Best ROC AUC: 0.5354\n",
      "Fold 1 - AUC: 0.9536\n",
      "Fold 2 - AUC: 0.9540\n",
      "Fold 3 - AUC: 0.9545\n",
      "Fold 4 - AUC: 0.9565\n",
      "Fold 5 - AUC: 0.9522\n",
      "Final Average AUC: 0.9542\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 避免多核心檢測錯誤的環境變數設定\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"6\"  # 使用 6 個核心\n",
    "\n",
    "# 讀取原始資料並隨機打亂\n",
    "data = pd.read_csv('./data/data.csv').sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 將資料分為 train.csv 和 test.csv\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['is_foliage'])\n",
    "train_data.to_csv('./data/train.csv', index=False)\n",
    "test_data.to_csv('./data/test.csv', index=False)\n",
    "\n",
    "print(\"資料已分割並儲存為 train.csv 和 test.csv\")\n",
    "\n",
    "# 讀取 train.csv 和 test.csv\n",
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# 特徵與目標變數\n",
    "X_train = train_data.drop(columns=['is_foliage'])\n",
    "y_train = train_data['is_foliage'].apply(lambda x: 1 if x == 1.0 else 0)\n",
    "\n",
    "X_test = test_data.drop(columns=['is_foliage'])\n",
    "y_test = test_data['is_foliage'].apply(lambda x: 1 if x == 1.0 else 0)\n",
    "\n",
    "# 確保特徵數據全為數值型\n",
    "X_train = X_train.select_dtypes(include=['float64', 'int64'])\n",
    "X_test = X_test.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# 使用 SMOTE 平衡 train.csv 的類別分佈\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 標準化數值特徵\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# 計算 scale_pos_weight\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "# 設定 XGBClassifier 模型\n",
    "model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='gpu_hist',  # 使用 GPU 加速\n",
    "    gpu_id=0,                # 指定使用第 0 塊 GPU\n",
    "    scale_pos_weight=scale_pos_weight  # 設定 scale_pos_weight\n",
    ")\n",
    "\n",
    "# 縮小參數範圍設定，用於 RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [20, 25, 40],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'n_estimators': [1000, 1500, 2000],\n",
    "    'subsample': [0.8, 0.85, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.85],\n",
    "    'gamma': [0.2, 0.3, 0.4],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2],\n",
    "}\n",
    "\n",
    "# 使用 RandomizedSearchCV 進行隨機搜尋\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=20,\n",
    "    cv=5, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=3, \n",
    "    verbose=1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 訓練模型並搜尋最佳參數\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 顯示最佳參數與最佳 ROC AUC\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best ROC AUC (Train CV): {random_search.best_score_:.4f}\")\n",
    "\n",
    "# 使用最佳參數訓練最終模型\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# 使用 test.csv 預測並計算 ROC AUC 分數\n",
    "y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"Test ROC AUC: {test_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
